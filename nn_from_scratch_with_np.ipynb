{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QeHDInzG6ae"
   },
   "source": [
    "# Neuronal Network from Scratch with Numpy\n",
    "In this notebook we develop a neural network from scratch using only Numpy. We develop the code based on the excellent [tutorial](https://victorzhou.com/blog/intro-to-neural-networks/) by Victor Zhou.\n",
    "\n",
    "## The Activation Function\n",
    "As activation function we use the sigmoid function $\\sigma(x) = \\frac{1}{1+e^{-x}}$. The derivative of the sigmoid function is $\\sigma'(x) = \\sigma(x)\\cdot (1-\\sigma(x))$.\n",
    "\n",
    "## The Data\n",
    "We have given persons with weight, height and gender. The neuronal network should now learn to predict the gender for a given weight and height.\n",
    "\n",
    "## The Network Architecture\n",
    "<img src=\"img/nn_1.png\" style=\"height:250px\">\n",
    "\n",
    "## Loss Function\n",
    "As loss-function we use the Mean Squared Error $L=\\frac{1}{n}\\sum_{i=1}^n (y-\\hat{y})^2$, with $\\hat{y}=o_1$ beeing the predicted labels (genders) and $y$ beeing the given labels of the training set.\n",
    "\n",
    "## Feed Forward\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    h_1 &= \\sigma(w_1 x_1 + w_2 x_2 + b_1)\\\\\n",
    "    h_2 &= \\sigma(w_3 x_1 + w_4 x_2 + b_2)\\\\\n",
    "    o_1 &= \\sigma(w_5 h_1 + w_6 h_2 + b_3)\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "## Gradient Decent\n",
    "Our goal is to understand how the costfunction $L(w_1,w_2,w_3,w_4,w_5,w_6,b_1,b_2,b_3)$ changes with respect to all the weights and biasis. That way we know which adjustments to those terms lead to the most efficient decrease of it.\n",
    "\n",
    "So let's start with $w_6$ and $w_5$. How do we have to change $w_6$ respective $w_5$ to decrease the cost function $L$? This question can be answered by the partial derivative $\\frac{\\partial L}{\\partial w_6}$ respective $\\frac{\\partial L}{\\partial w_5}$.\n",
    "\n",
    "$\n",
    "  \\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_6} &= \\color{red}{\\frac{\\partial L}{\\partial \\hat{y}}} \\color{green}{\\frac{\\partial \\hat{y}}{\\partial w_6}}\\\\\n",
    "    \\color{red} {\\frac{\\partial L}{\\partial \\hat{y}}} &= \\frac{\\partial (y-\\hat{y})^2}{\\partial \\hat{y}} = -2(y-\\hat{y})\\\\\n",
    "    \\color{green}{\\frac{\\partial \\hat{y}}{w_6}} &= \\frac{\\partial \\sigma(w_5 h_1 + w_6 h_2 + b_3)}{\\partial w_6} = h_2 \\sigma'(w_5 h_1 + w_6 h_2 + b_3)\\\\\n",
    "    \\rightarrow \\frac{\\partial L}{\\partial w_6} &= \\color{red}{-2(y-\\hat{y})} \\color{green}{h_2 \\sigma'(w_5 h_1 + w_6 h_2 + b_3)}\n",
    "  \\end{aligned}\n",
    "$\n",
    "\n",
    "Similarly $\\frac{\\partial L}{\\partial w_5}$ can be derived.\n",
    "\n",
    "$\n",
    "  \\frac{\\partial L}{\\partial w_5} = -2(y - \\hat{y}) h_1 \\sigma'(w_5 h_1 + w_6 h_2 + b_3)\n",
    "$<br><br>\n",
    "\n",
    "In the next step we must now derive $\\frac{\\partial L}{\\partial w_4}, \\frac{\\partial L}{\\partial w_3},\\frac{\\partial L}{\\partial w_2} $ and $\\frac{\\partial L}{\\partial w_1}$.\n",
    "\n",
    "$\n",
    "  \\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_4} &= \\color{red}{\\frac{\\partial L}{\\partial \\hat{y}}} \\color{green}{\\frac{\\partial \\hat{y}}{\\partial w_4}}\\\\\n",
    "    \\color{red}{\\frac{\\partial L}{\\partial \\hat{y}}} &= \\frac{\\partial (y-\\hat{y})^2}{\\partial \\hat{y}} = -2(y-\\hat{y})\\\\\n",
    "    \\color{green}{\\frac{\\partial \\hat{y}}{\\partial w_4}} &\\overset{*}{=} \\color{orange}{\\frac{\\partial \\hat{y}}{h_2}} \\color{blue}{\\frac{\\partial h_2}{\\partial w_4}}\\\\\n",
    "    \\color{orange}{\\frac{\\partial \\hat{y}}{\\partial h_2}} &= \\frac{\\partial \\sigma(w_5 h_1 + w_6 h_2 + b_3)}{\\partial h_2} = w_6 \\sigma'(w_5 h_1 + w_6 h_2 + b_3)\\\\\n",
    "    \\color{blue}{\\frac{\\partial h_2}{\\partial w_4}} &= x_2 \\sigma'(w_3 x_1 + w_4 x_2 + b_2)\\\\\n",
    "    \\rightarrow \\frac{\\partial L}{\\partial w_4} &= \\color{red}{-2(y-\\hat{y})}\\color{orange}{w_6 \\sigma'(w_5 h_1 + w_6 h_2 + b_3)}\\color{blue}{x_2 \\sigma'(w_3 x_1 + w_4 x_2 + b_2)}\n",
    "  \\end{aligned}\n",
    "$\n",
    "\n",
    "$*)$ Because $w_4$ only influences $h_2$ and not $h_1$.\n",
    "\n",
    "Similarly $\\frac{\\partial L}{\\partial w_3},\\frac{\\partial L}{\\partial w_2} $ and $\\frac{\\partial L}{\\partial w_1}$ can be derived.\n",
    "\n",
    "$\n",
    "  \\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial w_3} &= -2(y-\\hat{y})w_6\\sigma'(w_5 h_1 + w_6 h_2 + b_3)x_1\\sigma'(w_3 x_1 + w_4 x_2 + b_2)\\\\\n",
    "    \\frac{\\partial L}{\\partial w_2} &= -2(y-\\hat{y})w_5\\sigma'(w_5 h_1 + w_6 h_2 + b_3)x_2\\sigma'(w_1 x_1 + w_2 x_2 + b_1)\\\\\n",
    "    \\frac{\\partial L}{\\partial w_1} &= -2(y-\\hat{y})w_5\\sigma'(w_5 h_1 + w_6 h_2 + b_3)x_1\\sigma'(w_1 x_1 + w_2 x_2 + b_1)\n",
    "  \\end{aligned}\n",
    "$<br><br>\n",
    "\n",
    "Finally we have to derive $\\frac{\\partial L}{\\partial b_3}$, $\\frac{\\partial L}{\\partial b_2}$ and $\\frac{\\partial L}{\\partial b_1}$. $\\frac{\\partial L}{\\partial b_3}$ can be derived similarly to $\\frac{\\partial L}{\\partial w_6}$.\n",
    "$\n",
    "  \\frac{\\partial L}{\\partial b_3} = -2(y-\\hat{y})\\sigma'(w_5 h_1 + w_6 h_2 + b_3)\n",
    "$\n",
    "\n",
    "And $\\frac{\\partial L}{\\partial b_2}$ can be derived similarly to $\\frac{\\partial L}{\\partial w_4}$ while $\\frac{\\partial L}{\\partial b_1}$ can be derived similarly to $\\frac{\\partial L}{\\partial w_2}$.\n",
    "$\n",
    "  \\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial b_2} &= -2(y-\\hat{y})w_6\\sigma'(w_5 h_1 + w_6 h_2 + b_3)\\sigma'(w_3 x_1 + w_4 x_2 + b_2)\\\\\n",
    "    \\frac{\\partial L}{\\partial b_1} &= -2(y-\\hat{y})w_5\\sigma'(w_5 h1 + w_6 h_2 + b_3)\\sigma'(w_1 x_1 + w_2 x_2 + b_1)\n",
    "  \\end{aligned}\n",
    "$\n",
    "\n",
    "## So, how do we train the Network? Backpropagation!\n",
    "We now have everything together to train our neural network. We use an algorithm called stochastic gradient descent (SGD) to calculate how to change our weights and biases to minimize loss. The SGD proceeds as follows.\n",
    "\n",
    "1. Chose randomly one sample from the trainingset.\n",
    "2. Update all weights and biases using the following equation. $\\eta$ is the learning rate which controls the training speed.\n",
    "\n",
    "$$\n",
    "  \\begin{aligned}\n",
    "    w_i &= w_i - \\eta \\frac{\\partial L}{w_i} \\text{ for all } i\\in[1,6]\\\\\n",
    "    b_j &= b_j - \\eta \\frac{\\partial L}{b_j} \\text{ for all } j\\in [1,3]\n",
    "  \\end{aligned}\n",
    "$$\n",
    "\n",
    "3. Go to step 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4ue9iKzdJFr"
   },
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pSs0TlBvdNEl"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CkKoCZip6CL1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KvM9m17odPps"
   },
   "source": [
    "### The Datasets & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1226,
     "status": "ok",
     "timestamp": 1578172309125,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -60
    },
    "id": "CWRNwcBmdDSA",
    "outputId": "dff4c62b-0d29-4fd4-fd0c-e96ddd04f270"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:\n",
      "      name  weight  height gender\n",
      "0    Alice     133      65      F\n",
      "1      Bob     160      72      M\n",
      "2  Charlie     152      70      M\n",
      "3    Diana     120      60      F\n",
      "\n",
      "prediction_data:\n",
      "    name  weight  height\n",
      "0  Emily     128      63\n",
      "1  Frank     155      68\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.DataFrame({'name':   ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "                              'weight': [133, 160, 152, 120],\n",
    "                              'height': [65, 72, 70, 60],\n",
    "                              'gender': ['F', 'M', 'M', 'F']})\n",
    "print('training_data:')\n",
    "print(training_data)\n",
    "\n",
    "# replace 'F' with 1 and 'M' with 0\n",
    "training_data['gender'].replace(to_replace=['F','M'], value=[1,0], inplace=True)\n",
    "\n",
    "# normalize weight and height\n",
    "for i in ['weight', 'height']:\n",
    "  training_data[i] = (training_data[i] - \n",
    "                             training_data[i].mean())\\\n",
    "                           / training_data[i].std()\n",
    "\n",
    "data = training_data[['weight', 'height']].to_numpy()\n",
    "\n",
    "all_y_trues = training_data['gender'].to_numpy()\n",
    "\n",
    "prediction_data = pd.DataFrame({'name':   ['Emily', 'Frank'],\n",
    "                                'weight': [128, 155],\n",
    "                                'height': [63, 68]})\n",
    "print('\\nprediction_data:')\n",
    "print(prediction_data)\n",
    "\n",
    "# normalize weight and height\n",
    "for i in ['weight', 'height']:\n",
    "  prediction_data[i] = (prediction_data[i] - \n",
    "                             prediction_data[i].mean())\\\n",
    "                           / prediction_data[i].std()\n",
    "\n",
    "pred_data = prediction_data.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NUObj772lW8Q"
   },
   "source": [
    "### Code for the Neuronal Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5230,
     "status": "ok",
     "timestamp": 1578172338584,
     "user": {
      "displayName": "jonas sander",
      "photoUrl": "",
      "userId": "00193758946461779324"
     },
     "user_tz": -60
    },
    "id": "xB8kx3j1480m",
    "outputId": "018db080-6ec7-4a6c-8961-f27f76c915a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.381\n",
      "Epoch 20 loss: 0.260\n",
      "Epoch 40 loss: 0.163\n",
      "Epoch 60 loss: 0.119\n",
      "Epoch 80 loss: 0.089\n",
      "Epoch 100 loss: 0.067\n",
      "Epoch 120 loss: 0.052\n",
      "Epoch 140 loss: 0.041\n",
      "Epoch 160 loss: 0.033\n",
      "Epoch 180 loss: 0.027\n",
      "Epoch 200 loss: 0.023\n",
      "Epoch 220 loss: 0.019\n",
      "Epoch 240 loss: 0.017\n",
      "Epoch 260 loss: 0.015\n",
      "Epoch 280 loss: 0.013\n",
      "Epoch 300 loss: 0.012\n",
      "Epoch 320 loss: 0.011\n",
      "Epoch 340 loss: 0.010\n",
      "Epoch 360 loss: 0.009\n",
      "Epoch 380 loss: 0.008\n",
      "Epoch 400 loss: 0.007\n",
      "Epoch 420 loss: 0.007\n",
      "Epoch 440 loss: 0.006\n",
      "Epoch 460 loss: 0.006\n",
      "Epoch 480 loss: 0.006\n",
      "Epoch 500 loss: 0.005\n",
      "Epoch 520 loss: 0.005\n",
      "Epoch 540 loss: 0.005\n",
      "Epoch 560 loss: 0.005\n",
      "Epoch 580 loss: 0.004\n",
      "Epoch 600 loss: 0.004\n",
      "Epoch 620 loss: 0.004\n",
      "Epoch 640 loss: 0.004\n",
      "Epoch 660 loss: 0.004\n",
      "Epoch 680 loss: 0.003\n",
      "Epoch 700 loss: 0.003\n",
      "Epoch 720 loss: 0.003\n",
      "Epoch 740 loss: 0.003\n",
      "Epoch 760 loss: 0.003\n",
      "Epoch 780 loss: 0.003\n",
      "Epoch 800 loss: 0.003\n",
      "Epoch 820 loss: 0.003\n",
      "Epoch 840 loss: 0.003\n",
      "Epoch 860 loss: 0.003\n",
      "Epoch 880 loss: 0.002\n",
      "Epoch 900 loss: 0.002\n",
      "Epoch 920 loss: 0.002\n",
      "Epoch 940 loss: 0.002\n",
      "Epoch 960 loss: 0.002\n",
      "Epoch 980 loss: 0.002\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily 0.9675447684627551\n",
      "Frank 0.04440184098894103\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "  fx = sigmoid(x)\n",
    "  return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "  # y_true and y_pred are numpy arrays of the same length.\n",
    "  return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class NeuralNetwork:\n",
    "  '''\n",
    "  A neural network with:\n",
    "    - 2 inputs\n",
    "    - a hidden layer with 2 neurons (h1, h2)\n",
    "    - an output layer with 1 neuron (o1)\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    # Weights\n",
    "    self.w1 = np.random.normal()\n",
    "    self.w2 = np.random.normal()\n",
    "    self.w3 = np.random.normal()\n",
    "    self.w4 = np.random.normal()\n",
    "    self.w5 = np.random.normal()\n",
    "    self.w6 = np.random.normal()\n",
    "\n",
    "    # Biases\n",
    "    self.b1 = np.random.normal()\n",
    "    self.b2 = np.random.normal()\n",
    "    self.b3 = np.random.normal()\n",
    "\n",
    "  def feedforward(self, x):\n",
    "    # x is a numpy array with 2 elements.\n",
    "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "    return o1\n",
    "\n",
    "  def train(self, data, all_y_trues):\n",
    "    '''\n",
    "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
    "    - all_y_trues is a numpy array with n elements.\n",
    "      Elements in all_y_trues correspond to those in data.\n",
    "    '''\n",
    "    learn_rate = 0.1\n",
    "    epochs = 1000 # number of times to loop through the entire dataset\n",
    "\n",
    "    # plot loss during training\n",
    "    fig1, ax1 = plt.subplots(figsize=(5,5))\n",
    "    ax1.set_title('Mean Square Loss Error')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      for x, y_true in zip(data, all_y_trues):\n",
    "        # --- Do a feedforward (we'll need these values later)\n",
    "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "        h1 = sigmoid(sum_h1)\n",
    "\n",
    "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "        h2 = sigmoid(sum_h2)\n",
    "\n",
    "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "        o1 = sigmoid(sum_o1)\n",
    "        y_pred = o1\n",
    "\n",
    "        # --- Calculate partial derivatives.\n",
    "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "        # Neuron o1\n",
    "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "        # Neuron h1\n",
    "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "        # Neuron h2\n",
    "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "        # --- Update weights and biases\n",
    "        # Neuron h1\n",
    "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "        # Neuron h2\n",
    "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "        # Neuron o1\n",
    "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "      # --- Calculate total loss\n",
    "      # get predictions for all samples from data\n",
    "      # apply feedforward along x axis\n",
    "      y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "      loss = mse_loss(all_y_trues, y_preds)\n",
    "      ax1.scatter(epoch, loss, marker='.', c='b')\n",
    "      if epoch % 20 == 0:\n",
    "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "    plt.show()\n",
    "\n",
    "# Train our neural network!\n",
    "network = NeuralNetwork()\n",
    "network.train(data, all_y_trues)\n",
    "\n",
    "# Make some predictions\n",
    "y_preds = np.apply_along_axis(network.feedforward, 1, pred_data[:,1:3])\n",
    "for i in range(len(y_preds)):\n",
    "  print(pred_data[i,0], y_preds[i])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nn_from_scratch_with_np.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
